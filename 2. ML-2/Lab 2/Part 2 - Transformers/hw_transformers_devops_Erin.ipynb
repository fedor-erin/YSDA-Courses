{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9f14c74d",
      "metadata": {
        "id": "9f14c74d"
      },
      "source": [
        "### Девопсная домашка по трансформерам\n",
        "\n",
        "![img](https://d35w6hwqhdq0in.cloudfront.net/521712556725591dcacec5bbdb32e047.png)\n",
        "\n",
        "Ваш главный квест на эту домашку - сделать свой простой сервис на трансформерах. Вот прям целый сервис: начиная с данных и заканчивая графическим интерфейсом где-то в интернете. Ваш сервис может решать либо стандартную задачу, либо что-то более дорогое лично вам.\n",
        "\n",
        "__Стандартная задача: классификатор статей.__ Нужно построить сервис который принимает название статьи и её abstract, и выдаёт наиболее вероятную тематику статьи: скажем, физика, биология или computer science. В интерфейсе должно быть можно ввести отдельно abstract, отдельно название -- и увидеть топ-95%* тематик, отсортированных по убыванию вероятности. Если abstract не ввели, нужно классифицировать статью только по названию. Ниже вас ждут инструкции и данные именно для этой задачи.\n",
        "\n",
        "<details><summary><u> Что значит Топ-95%?</u></summary>\n",
        "    Нужно выдавать темы по убыванию вероятности, пока их суммарная вероятность не превысит 95%. В зависимости от предсказанной вероятности, это может быть одна или более тем. Например, если модель предсказала вероятности [4%, 20%, 60%, 2%, 14%], нужно вывести 3 топ-3 класса. Если один из классов имеет вероятность 96%, достаточно вывести один этот класс.\n",
        "</details>\n",
        "\n",
        "Альтернативно, вы можете отважиться сделать что-то своё, на данных из интернета или своих собственных. В вашей задаче обязательно должно быть _оправданное_ использование трансформеров. Использовать ML чтобы переводить часовые пояса - плохой план.\n",
        "\n",
        "Achtung: трансформеры круты, но не всемогущи. Далеко не любую задачу можно решить ощутимо лучше рандома. Для калибровки, вот несколько примеров решаемых задач (всё кликабельно):\n",
        "\n",
        "\n",
        "<details><summary> - <b>[medium]</b> <u>Сгенерировать youtube-комментарии по _ссылке_ на видео</u></summary>\n",
        "    Всё просто, юзер постит ссылку на видео - вы его комментируете. Можно заранее обусловиться что видео только на английском или на русском. Нужно сочинить _несколько_ комментариев. Kudos если вместе с основным комментарием вы порождаете юзернеймы и-или ответы на него.\n",
        "    \n",
        "    Датасет для файнтюна можно [взять с kaggle](https://www.kaggle.com/tanmay111/youtube-comments-sentiment-analysis/data?select=UScomments.csv) или [собрать самостоятельно](https://towardsdatascience.com/how-to-build-your-own-dataset-of-youtube-comments-39a1e57aade).\n",
        "    \n",
        "    В качестве основной модели можно использовать [GPT-2 large](https://huggingface.co/gpt2-large). Вот как её файнтюнить: https://tinyurl.com/gpt2-finetune-colab . Если хотите больше - можно взять что-то из творчества https://huggingface.co/EleutherAI . Например, вот [тут](https://tinyurl.com/gpt-j-8bit) есть пример как файнтюнить GPT-J-6B (в 8 раз больше gpt2-large). Однако, этим стоит заниматься уже после того, как у вас заработал базовый сценарий с GPT2-large или даже base.\n",
        "    \n",
        "    В итоговом сервисе можно дать пользователю вариировать параметры генерации: температура или top-p, если сэмплинг; beam size и length penalty, если beam search; сколько комментариев сгенерировать, etc. Отдельный респект если ваш код будет выводить комментарий по одному слову, прямо в процессе генерёжки - чтобы пользователь не ждал пока вы настругаете абзац целиком.\n",
        "</details>\n",
        "\n",
        "<details><summary> - <b>[medium]</b> <u>Предсказать зарплату по профилю (симулятор Дудя).</u></summary>\n",
        "    Note: <a href=https://www.youtube.com/watch?v=_MxoRsdBCtw>Причём тут Дудь?</a>\n",
        "    \n",
        "    Главная сложность задачи - достать хорошие данные. Если хороших данных не случилось - можно и трешовые :) Задание всё-таки про технологии а не про продукт. Для начала можно взять подмножество фичей [отсюда](https://www.kaggle.com/c/job-salary-prediction/data), которые вы можете восстановить из профиля linkedin - название профессии и компании. Название компании лучше заменить на фичи из открытых источников: сфера деятельности, размер, етц.\n",
        "    \n",
        "    А дальше файнтюним на этом BERT / T5 и радуемся. Ну или хотя бы смеёмся.\n",
        "</details>\n",
        "\n",
        "\n",
        "<details><summary> - <b>[hard]</b> <u>Мнения с географической окраской.</u></summary>\n",
        "    \n",
        "    Сервис который принимает на вход тему (хэштег или ключевую фразу) и рисует карту мира, где в каждом регионе показано, с какой эмоциональной окраской о ней высказываются в социальных сетях. В качестве социальной сети можно взять VK/twitter, в случая VK ожидается детализация не по странам, а по городам стран бывшего СССР.\n",
        "    \n",
        "    В минимальном варианте достаточно определять тональность твита в режиме \"позитивно-негативно\", зафайнтюнив условный BERT/T5 на одном из десятков {vk/twitter} sentiment classification датасетах. Географическую привязку можно получить из профиля пользователя. А дальше осталось собрать данные по странам и регионам.\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "<details><summary> - <b>[very hard]</b> <u>Найти статью википедии по фото предмета статьи</u></summary>\n",
        "\n",
        "    Чтобы можно было сфотать какую-нибудь неведомую чешуйню на телефон и получить сумму человеческих знаний о ней в форме вики-статьи.\n",
        "    \n",
        "    В качестве функции потерь можно использовать contrastive loss. Этот лосс неплохо описан в статье [CLIP](https://arxiv.org/abs/2103.00020). Вместо обучения с нуля предлагается взять, собственно, CLIP (text transformer + image transformer) отсюда: https://huggingface.co/docs/transformers/model_doc/clip. Модель будет сопоставлять каждой статьи и \n",
        "    \n",
        "    Данные для этого квеста можно собрать через API википедии: вики-статьи о предметах обычно содержит фото этого объекта и, собственно, текст статьи. Советуем собрать как минимум 10^4 пар картинка-статья. Картинки советуем дополнительно аугментировать как минимум стандартными картиночными аугами, как максимум - поиском похожих картинок в интернете / imagenet-е по тому же CLIP image encoder-у, но с исходными весами.\n",
        "    \n",
        "    На время отладки интерфейса рекомендуем ограничиться небольшим списком статьей: условно, кошечки, собачки, птички, гаечные ключи, машины. Как станет понятно что оно работает \"на кошках\", можно расширить этот список до \"всех статей таких-то категорий\". Эмбединги статей лучше предпосчитать в файл. Если долго их перебирать - можно (но необязательно) воспользоваться быстрым поиском соседей, e.g. [faiss](https://github.com/facebookresearch/faiss) HNSW.\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f8bb3df",
      "metadata": {
        "id": "8f8bb3df"
      },
      "source": [
        "### Как научить классификатор статей?\n",
        "\n",
        "Данные для классификации статей можно скачать, например, [отсюда](https://www.kaggle.com/neelshah18/arxivdataset/). В этих данных есть заголовок и abstract статьи, а ещё поле __\"tag\"__: тематика статьи [по таксономии arxiv.org](https://arxiv.org/category_taxonomy). Вы можете расширить выборку, добавив в неё статьи за 2019-2022 годы. Для этого можно [использовать arxiv API](https://github.com/lukasschwab/arxiv.py), самостоятельно распарсить arxiv с помощью [beautifulsoup](https://pypi.org/project/beautifulsoup4/), или поискать другие датасеты на kaggle, huggingface, etc.\n",
        "\n",
        "Когда данные собраны (и аккуратно нарезаны на train/test), можно что-нибудь и обучить. Мы советуем использовать для этого библиотеку `transformers`. Советуем, но не заставляем: если хочется, можно взять [fairseq roberta](https://github.com/pytorch/fairseq/blob/main/examples/roberta), [google t5](https://github.com/google-research/text-to-text-transfer-transformer) или даже написать всё с нуля.\n",
        "\n",
        "Базовый туториал по transformers с семинара можно найти на [семинаре](https://colab.research.google.com/drive/1QOnED8bRHvclTLNFSNvoO-lkcU01qyNu). Более детальный гайд по Trainer ждёт вас [в доках](https://huggingface.co/docs/transformers/training).\n",
        "\n",
        "Начать лучше с простой модели, такой как [`distilbert-base-cased`](https://huggingface.co/distilbert-base-cased). Когда вы будете понимать, какие значения accuracy ожидать от базовой модели, можно поискать что-то получше. Два очевидных направления улучшения: (1) сильнее модель T5 или deberta v3, или (2) близкие данные, например взять модель которую предобучили на том же arxiv. И то и другое удобно [искать здесь](https://huggingface.co/models)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "CjosJ7tkpVON"
      },
      "id": "CjosJ7tkpVON"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets"
      ],
      "metadata": {
        "id": "vYK3uVNYpVFC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abf7c9eb-6cf1-4e3f-d556-fd9b65dfa9b3"
      },
      "id": "vYK3uVNYpVFC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.8 MB 4.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 325 kB 52.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 28.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 50.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 44.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 52.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 39.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 134 kB 50.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 53.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 2.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 52.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 51.1 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter \n",
        "\n",
        "import torch\n",
        "from datasets import Dataset, load_metric\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments"
      ],
      "metadata": {
        "id": "oKYIwffMPN85"
      },
      "id": "oKYIwffMPN85",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Загрузка данных"
      ],
      "metadata": {
        "id": "Y6VarZCKKqia"
      },
      "id": "Y6VarZCKKqia"
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузил ./kaggle.json с \"Kaggle -> Account -> Generate New API Token\"\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download -d neelshah18/arxivdataset\n",
        "! unzip arxivdataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FyRXpn5MPDP",
        "outputId": "8d066ddb-2743-466d-b912-6f4dd60d5f08"
      },
      "id": "1FyRXpn5MPDP",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading arxivdataset.zip to /content\n",
            "\r  0% 0.00/18.3M [00:00<?, ?B/s]\n",
            "\r100% 18.3M/18.3M [00:00<00:00, 219MB/s]\n",
            "Archive:  arxivdataset.zip\n",
            "  inflating: arxivData.json          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('arxivData.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print('Всего статей:', len(data), '\\n')\n",
        "data[0]"
      ],
      "metadata": {
        "id": "TZoyvtsuOsd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b19c7f04-87cb-488f-a5c5-ab2e4fe1f474"
      },
      "id": "TZoyvtsuOsd6",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Всего статей: 41000 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'author': \"[{'name': 'Ahmed Osman'}, {'name': 'Wojciech Samek'}]\",\n",
              " 'day': 1,\n",
              " 'id': '1802.00209v1',\n",
              " 'link': \"[{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1802.00209v1', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1802.00209v1', 'type': 'application/pdf', 'title': 'pdf'}]\",\n",
              " 'month': 2,\n",
              " 'summary': 'We propose an architecture for VQA which utilizes recurrent layers to\\ngenerate visual and textual attention. The memory characteristic of the\\nproposed recurrent attention units offers a rich joint embedding of visual and\\ntextual features and enables the model to reason relations between several\\nparts of the image and question. Our single model outperforms the first place\\nwinner on the VQA 1.0 dataset, performs within margin to the current\\nstate-of-the-art ensemble model. We also experiment with replacing attention\\nmechanisms in other state-of-the-art models with our implementation and show\\nincreased accuracy. In both cases, our recurrent attention mechanism improves\\nperformance in tasks requiring sequential or relational reasoning on the VQA\\ndataset.',\n",
              " 'tag': \"[{'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.NE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\",\n",
              " 'title': 'Dual Recurrent Attention Units for Visual Question Answering',\n",
              " 'year': 2018}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Пример тегов одной статьи:', [t['term'] for t in eval(data[0]['tag'])])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-eW1R6lPMU1",
        "outputId": "d1171654-7f67-451a-e759-dbf98f264cf1"
      },
      "id": "X-eW1R6lPMU1",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Пример тегов одной статьи: ['cs.AI', 'cs.CL', 'cs.CV', 'cs.NE', 'stat.ML']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Подготовка данных для обучения"
      ],
      "metadata": {
        "id": "KYqkUZNuPMon"
      },
      "id": "KYqkUZNuPMon"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Формализуем задачу:\n",
        "* Задача многоклассовой классификации, вход - текст + один класс (первый в списке tag'ов), выход - вероятности по классам\n",
        "* Объект предсказания - текст = конкатенация строк названия статьи и abstract'а\n",
        "* Модель - fine-tuned трансформер на данных Arxiv\n",
        "* Данные сильно несбалансированны, сделаем балансировку\n",
        "* Для редких тэгов (<20 в датасете) сделаем oversampling до 20 штук\n",
        "* Для мажорного класса (Computer Science) сделаем downsampling до размера второго класса по размеру (Statistics)"
      ],
      "metadata": {
        "id": "yD1RQapSgEeX"
      },
      "id": "yD1RQapSgEeX"
    },
    {
      "cell_type": "code",
      "source": [
        "# все возможные категории\n",
        "categories = {\n",
        "    'cs': 'Computer Science',\n",
        "    'econ': 'Economics',\n",
        "    'eess': 'Electrical Engineering and Systems Science',\n",
        "    'math': 'Mathematics',\n",
        "    'astro-ph': 'Physics',\n",
        "    'cond-mat': 'Physics',\n",
        "    'gr-qc': 'Physics',\n",
        "    'hep-ex': 'Physics',\n",
        "    'hep-lat': 'Physics',\n",
        "    'hep-ph': 'Physics',\n",
        "    'hep-th': 'Physics',\n",
        "    'math-ph': 'Physics',\n",
        "    'nlin': 'Physics',\n",
        "    'nucl-ex': 'Physics',\n",
        "    'nucl-th': 'Physics',\n",
        "    'physics': 'Physics',\n",
        "    'quant-ph': 'Physics',\n",
        "    'q-bio': 'Quantitative Biology',\n",
        "    'q-fin': 'Quantitative Finance',\n",
        "    'stat': 'Statistics',\n",
        "}\n",
        "\n",
        "def create_dataset(data: list) -> list:\n",
        "    \"\"\"\n",
        "    Парсим json, извлекаем текст + один основной тег\n",
        "    \"\"\"\n",
        "    dataset = []\n",
        "    print('Initial articles number:', len(data))\n",
        "    for article in tqdm(data):\n",
        "        main_tag = eval(article['tag'])[0]['term'].split('.')[0]\n",
        "        if main_tag in categories:\n",
        "            dataset.append({'tag': categories[main_tag], \n",
        "                            'text': article['title'] + \" \" + article['summary']})\n",
        "    print('\\nPreprocessed and cleaned for dataset:', len(dataset))\n",
        "    return dataset\n",
        "\n",
        "def ovesample_rare_tags(dataset: list, oversample_size=20) -> list:\n",
        "    \"\"\"\n",
        "    Для редких категорий оверсемплим их же с повторением\n",
        "    \"\"\"\n",
        "    ds = dataset.copy()\n",
        "    frequency = Counter([article['tag'] for article in ds])\n",
        "    rare_tags = []\n",
        "\n",
        "    for tag, count in frequency.items():\n",
        "        if count < oversample_size:\n",
        "            rare_tags.append(tag)\n",
        "\n",
        "    rare_tags_texts = {tag: [] for tag in rare_tags}\n",
        "    for article in ds:\n",
        "        if article['tag'] in rare_tags:\n",
        "            rare_tags_texts[article['tag']].append(article['text'])\n",
        "\n",
        "    oversamples = []\n",
        "    for rare_tag in rare_tags:\n",
        "        need_to_generate = oversample_size - len(rare_tags_texts[rare_tag])\n",
        "        samples = np.random.choice(rare_tags_texts[rare_tag], need_to_generate)\n",
        "        oversamples += [{'tag': rare_tag, 'text': sample} for sample in samples]\n",
        "    \n",
        "    ds += oversamples\n",
        "    return ds\n",
        "\n",
        "def downsample_major_class(dataset: list) -> list:\n",
        "    \"\"\"\n",
        "    На Computer Science приходится более 80% статей, сравняем их кол-во\n",
        "    со второй по величине категорией - Statistics, случайно выбрав статьи CS\n",
        "    \"\"\"\n",
        "    cs_articles = [article for article in dataset if article['tag'] == 'Computer Science']\n",
        "    other_articles = [article for article in dataset if article['tag'] != 'Computer Science']\n",
        "    print(dataset[0])\n",
        "    n_samples = Counter([article['tag'] for article in dataset])['Statistics']\n",
        "    cs_sample = list(np.random.choice(cs_articles, n_samples))\n",
        "    new_dataset = other_articles + cs_sample\n",
        "    np.random.shuffle(new_dataset)\n",
        "    return new_dataset"
      ],
      "metadata": {
        "id": "0CAp3b09g9O5"
      },
      "id": "0CAp3b09g9O5",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = create_dataset(data)\n",
        "ds = ovesample_rare_tags(ds)\n",
        "ds = downsample_major_class(ds)\n",
        "\n",
        "Counter([article['tag'] for article in ds]).most_common()"
      ],
      "metadata": {
        "id": "jRqfhpNrjYw7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64730072-9629-4fd4-d0d4-517e98fa8b19"
      },
      "id": "jRqfhpNrjYw7",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial articles number: 41000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 41000/41000 [00:01<00:00, 34775.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preprocessed and cleaned for dataset: 40888\n",
            "{'tag': 'Computer Science', 'text': 'Dual Recurrent Attention Units for Visual Question Answering We propose an architecture for VQA which utilizes recurrent layers to\\ngenerate visual and textual attention. The memory characteristic of the\\nproposed recurrent attention units offers a rich joint embedding of visual and\\ntextual features and enables the model to reason relations between several\\nparts of the image and question. Our single model outperforms the first place\\nwinner on the VQA 1.0 dataset, performs within margin to the current\\nstate-of-the-art ensemble model. We also experiment with replacing attention\\nmechanisms in other state-of-the-art models with our implementation and show\\nincreased accuracy. In both cases, our recurrent attention mechanism improves\\nperformance in tasks requiring sequential or relational reasoning on the VQA\\ndataset.'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Computer Science', 4782),\n",
              " ('Statistics', 4782),\n",
              " ('Mathematics', 612),\n",
              " ('Physics', 467),\n",
              " ('Quantitative Biology', 320),\n",
              " ('Electrical Engineering and Systems Science', 75),\n",
              " ('Quantitative Finance', 30),\n",
              " ('Economics', 20)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [article['text'] for article in ds]\n",
        "labels = [article['tag'] for article in ds]\n",
        "dataset = Dataset.from_dict({'text': texts, 'label': labels}).train_test_split(0.2)\n",
        "dataset"
      ],
      "metadata": {
        "id": "gcwAw7ck9dIa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "689e649f-9784-41c9-a53c-a5d7ef61b669"
      },
      "id": "gcwAw7ck9dIa",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 8870\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 2218\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "287XirTB8GHW",
        "outputId": "6f4c5662-f2f9-4667-e795-6c8bed611425"
      },
      "id": "287XirTB8GHW",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': 'Statistics',\n",
              " 'text': 'The Marginal Value of Adaptive Gradient Methods in Machine Learning Adaptive optimization methods, which perform local optimization with a metric\\nconstructed from the history of iterates, are becoming increasingly popular for\\ntraining deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We\\nshow that for simple overparameterized problems, adaptive methods often find\\ndrastically different solutions than gradient descent (GD) or stochastic\\ngradient descent (SGD). We construct an illustrative binary classification\\nproblem where the data is linearly separable, GD and SGD achieve zero test\\nerror, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to\\nhalf. We additionally study the empirical generalization capability of adaptive\\nmethods on several state-of-the-art deep learning models. We observe that the\\nsolutions found by adaptive methods generalize worse (often significantly\\nworse) than SGD, even when these solutions have better training performance.\\nThese results suggest that practitioners should reconsider the use of adaptive\\nmethods to train neural networks.'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {idx: label for idx, label in enumerate(sorted(set(labels)))}\n",
        "label2id = {label: idx for idx, label in enumerate(sorted(set(labels)))}\n",
        "label2id"
      ],
      "metadata": {
        "id": "BVmx01cQ8F_M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da9af5e3-c49a-48af-8bee-67a3a10d133f"
      },
      "id": "BVmx01cQ8F_M",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Computer Science': 0,\n",
              " 'Economics': 1,\n",
              " 'Electrical Engineering and Systems Science': 2,\n",
              " 'Mathematics': 3,\n",
              " 'Physics': 4,\n",
              " 'Quantitative Biology': 5,\n",
              " 'Quantitative Finance': 6,\n",
              " 'Statistics': 7}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tune трансформера"
      ],
      "metadata": {
        "id": "R43AsRZfjYPk"
      },
      "id": "R43AsRZfjYPk"
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'distilbert-base-cased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
        "                                                           num_labels=len(label2id)).to(\"cuda\")"
      ],
      "metadata": {
        "id": "arRXrtKSjYCe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e68aeec-5dcc-4d68-fbe4-bc3387d69178"
      },
      "id": "arRXrtKSjYCe",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
            "Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-cased\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/distilbert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/ba377304984dc63e3ede0e23a938bbbf04d5c3835b66d5bb48343aecca188429.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/acb5c2138c1f8c84f074b86dafce3631667fccd6efcb1a7ea1320cf75c386a36.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "loading file https://huggingface.co/distilbert-base-cased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/distilbert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/81e970e5e6ec68be12da0f8f3b2f2469c78d579282299a2ea65b4b7441719107.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
            "Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-cased\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
            "Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-cased\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7\n",
            "  },\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
            "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(examples):\n",
        "    text = examples['text']\n",
        "    encoding = tokenizer(text, padding=\"max_length\", truncation=True)\n",
        "    encoding['label'] = [label2id[x] for x in examples['label']]\n",
        "    return encoding\n",
        "\n",
        "dataset_encodings = dataset.map(preprocess, batched=True, remove_columns=['text'])\n",
        "dataset_encodings.set_format(\"torch\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "b3efc33e5247459bb8da664ecd8d2fd3",
            "e7cf0e45cca74f86b1dfca1948ad1abc",
            "1cdef38128d245d5a56c6cd2e05d7b45",
            "da2e20ec8d444d03b5195a70e2ba37b5",
            "a43c997f6ef84a5eb70c0643f0857edf",
            "ac44ad5c0c6c413f9e156ba64965ae3b",
            "7b352c00b1c34a0888fbb08026aab67c",
            "abb931957b0d474eb4da33dda4a4dc40",
            "998de46b427546689338992f20047277",
            "97bbc008970a4639bc5cdcd32c90344e",
            "2fc8ee670168419594b5a5c100ce7a3b",
            "c0824eb29213403e9a00f473f02846df",
            "c77be41a1d114e02a41c6cf6de759a85",
            "588e0f2b7f834baeb89f4f4f60c8b9c1",
            "48c476d27c9b47e3b84609c7e6af326f",
            "ec136ccd3eb44544b031a0d9e5e1d204",
            "f750c1adb4c1446b96948acb9d013875",
            "ed44db8295f14d51806993c7af894545",
            "7e547fddd3a84595b4dd58db026fc663",
            "555420acf21e452d8f9ba9a82e3f828f",
            "1bf76ee485124fb989d121412986b5e5",
            "4bbf8a66589b4bcf94466848a244cb9c"
          ]
        },
        "id": "fPH7orHYApQG",
        "outputId": "8c31e1a8-e52e-45e2-f98f-c8b313d1e84d"
      },
      "id": "fPH7orHYApQG",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/9 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b3efc33e5247459bb8da664ecd8d2fd3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0824eb29213403e9a00f473f02846df"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metric = load_metric('accuracy')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "mOIRP9YqFjlj"
      },
      "id": "mOIRP9YqFjlj",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    weight_decay=0.01,\n",
        "    # eval_steps=50,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    metric_for_best_model=\"accuracy\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_encodings[\"train\"],\n",
        "    eval_dataset=dataset_encodings[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "nChy1z6lohzZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7a4e062-0503-4b1a-e966-c626a765f572"
      },
      "id": "nChy1z6lohzZ",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# пример inference\n",
        "outputs = model(input_ids=dataset_encodings['train']['input_ids'][0].unsqueeze(0).to(\"cuda\"), \n",
        "                labels=dataset_encodings['train'][0]['label'].unsqueeze(0).to(\"cuda\"))\n",
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDR6lbe3KpTA",
        "outputId": "b04a506a-c289-4890-83f2-3ae618ebe6a9"
      },
      "id": "vDR6lbe3KpTA",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequenceClassifierOutput([('loss',\n",
              "                           tensor(2.1112, device='cuda:0', grad_fn=<NllLossBackward0>)),\n",
              "                          ('logits',\n",
              "                           tensor([[-0.0183,  0.0796, -0.0679, -0.1062, -0.0339,  0.0012, -0.1236, -0.0726]],\n",
              "                                  device='cuda:0', grad_fn=<AddmmBackward0>))])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "ezbZL8Mgohk4",
        "outputId": "47387015-0ef0-4488-9eba-5ec1029fc402"
      },
      "id": "ezbZL8Mgohk4",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 8870\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1110\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1110' max='1110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1110/1110 30:25, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.929600</td>\n",
              "      <td>0.772946</td>\n",
              "      <td>0.710550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.667700</td>\n",
              "      <td>0.721151</td>\n",
              "      <td>0.740307</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-500\n",
            "Configuration saved in ./results/checkpoint-500/config.json\n",
            "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2218\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./results/checkpoint-1000\n",
            "Configuration saved in ./results/checkpoint-1000/config.json\n",
            "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in ./results/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-1000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2218\n",
            "  Batch size = 32\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1110, training_loss=0.7800899402515308, metrics={'train_runtime': 1827.1968, 'train_samples_per_second': 9.709, 'train_steps_per_second': 0.607, 'total_flos': 2350223102607360.0, 'train_loss': 0.7800899402515308, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model('model.pt')"
      ],
      "metadata": {
        "id": "T0Ehj_fEOBEp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12437d75-7483-4433-8672-8a1b09cb139f"
      },
      "id": "T0Ehj_fEOBEp",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to model.pt\n",
            "Configuration saved in model.pt/config.json\n",
            "Model weights saved in model.pt/pytorch_model.bin\n",
            "tokenizer config file saved in model.pt/tokenizer_config.json\n",
            "Special tokens file saved in model.pt/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r model.zip model.pt"
      ],
      "metadata": {
        "id": "sFDUUIsQeB93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8ecd8e8-ad7b-462e-cab8-a48f05737b43"
      },
      "id": "sFDUUIsQeB93",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: model.pt/ (stored 0%)\n",
            "  adding: model.pt/training_args.bin (deflated 48%)\n",
            "  adding: model.pt/special_tokens_map.json (deflated 40%)\n",
            "  adding: model.pt/vocab.txt (deflated 49%)\n",
            "  adding: model.pt/tokenizer_config.json (deflated 39%)\n",
            "  adding: model.pt/pytorch_model.bin (deflated 8%)\n",
            "  adding: model.pt/config.json (deflated 54%)\n",
            "  adding: model.pt/tokenizer.json (deflated 70%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "metadata": {
        "id": "9NUJCdMLcyKP"
      },
      "id": "9NUJCdMLcyKP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Сохраним модель на Drive для случая перезапуска Colab\n",
        "!cp model.zip drive/MyDrive/model.zip"
      ],
      "metadata": {
        "id": "779Czd1riB9f"
      },
      "id": "779Czd1riB9f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Предсказание обученной моделью"
      ],
      "metadata": {
        "id": "JNep-OfUQy2n"
      },
      "id": "JNep-OfUQy2n"
    },
    {
      "cell_type": "code",
      "source": [
        "# Подгрузим сохраненную модель с Drive, если Colab перезапускался\n",
        "# from google.colab import drive\n",
        "# drive.mount('drive')"
      ],
      "metadata": {
        "id": "Gn-_gQ2UKP7W"
      },
      "id": "Gn-_gQ2UKP7W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL_PATH = 'drive/MyDrive/model.pt'\n",
        "MODEL_PATH = 'model.pt'"
      ],
      "metadata": {
        "id": "-N686O9YKPyw"
      },
      "id": "-N686O9YKPyw",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)"
      ],
      "metadata": {
        "id": "3CBT58qSOA8I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e38ddc5a-2952-4d3e-90ba-8e648dfd8c87"
      },
      "id": "3CBT58qSOA8I",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Didn't find file model.pt/added_tokens.json. We won't load it.\n",
            "loading file model.pt/vocab.txt\n",
            "loading file model.pt/tokenizer.json\n",
            "loading file None\n",
            "loading file model.pt/special_tokens_map.json\n",
            "loading file model.pt/tokenizer_config.json\n",
            "loading configuration file model.pt/config.json\n",
            "Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"model.pt\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7\n",
            "  },\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading weights file model.pt/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
            "\n",
            "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at model.pt.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_article_category(tokenizer, model, label2id, text, only_top95=False):\n",
        "    encoding = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    encoding = {k: v for k, v in encoding.items() if k in ['input_ids', 'attention_mask']}\n",
        "    preds = model(**encoding)\n",
        "    probs = preds.logits.softmax(dim=-1)[0].tolist()\n",
        "    predictions = {label: round(prob, 3) for (label, prob) in zip(label2id.keys(), probs)}\n",
        "    predictions = sorted(predictions.items(), key=lambda x: -x[1])\n",
        "    if only_top95:\n",
        "        top95_perc_index = list(np.cumsum([x[1] for x in predictions]) <= 0.95).index(False)\n",
        "        predictions = predictions[:top95_perc_index+1]\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "GGpNlQtBOSJU"
      },
      "id": "GGpNlQtBOSJU",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = dataset['test']['text'][3]\n",
        "print('True:', dataset['test']['label'][3], '\\n')\n",
        "print(text, '\\n')\n",
        "print('Predicted: \\n')\n",
        "get_article_category(tokenizer, model, label2id, text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WV0RxLEQxsrI",
        "outputId": "b51b5676-9db7-4a6e-d808-0cdd09bc1646"
      },
      "id": "WV0RxLEQxsrI",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True: Physics \n",
            "\n",
            "Notes on information geometry and evolutionary processes In order to analyze and extract different structural properties of\n",
            "distributions, one can introduce different coordinate systems over the manifold\n",
            "of distributions. In Evolutionary Computation, the Walsh bases and the Building\n",
            "Block Bases are often used to describe populations, which simplifies the\n",
            "analysis of evolutionary operators applying on populations. Quite independent\n",
            "from these approaches, information geometry has been developed as a geometric\n",
            "way to analyze different order dependencies between random variables (e.g.,\n",
            "neural activations or genes).\n",
            "  In these notes I briefly review the essentials of various coordinate bases\n",
            "and of information geometry. The goal is to give an overview and make the\n",
            "approaches comparable. Besides introducing meaningful coordinate bases,\n",
            "information geometry also offers an explicit way to distinguish different order\n",
            "interactions and it offers a geometric view on the manifold and thereby also on\n",
            "operators that apply on the manifold. For instance, uniform crossover can be\n",
            "interpreted as an orthogonal projection of a population along an m-geodesic,\n",
            "monotonously reducing the theta-coordinates that describe interactions between\n",
            "genes. \n",
            "\n",
            "Predicted: \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Physics', 0.688),\n",
              " ('Computer Science', 0.165),\n",
              " ('Quantitative Biology', 0.064),\n",
              " ('Mathematics', 0.046),\n",
              " ('Statistics', 0.027),\n",
              " ('Electrical Engineering and Systems Science', 0.005),\n",
              " ('Economics', 0.003),\n",
              " ('Quantitative Finance', 0.002)]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4032a07",
      "metadata": {
        "id": "f4032a07"
      },
      "source": [
        "### Научил, и что теперь?\n",
        "\n",
        "А теперь нужно сделать так, чтобы ваша обученная модель отвечала на запросы в интернете. Как и на прошлом этапе, вы можете сделать это кучей разных способов: от простого но медленного [streamlit](https://streamlit.io/) / [gradio](https://gradio.app/), минуя [TorchServe](https://pytorch.org/serve/) с [Triton/TensorRT](https://developer.nvidia.com/nvidia-triton-inference-server), и заканчивая экспортом модели в javascript с помощью [TensorFlow.js](https://www.tensorflow.org/js/tutorials) / [ONNX.js](https://github.com/elliotwaite/pytorch-to-javascript-with-onnx-js). Подробнее про этот зоопарк вы можете услышать из курса Efficient DL (4 семестр), а пока мы разберём streamlit.\n",
        "\n",
        "\n",
        "Общая идея [streamlit](streamlit.io): вы описываете внешний вид приложения на питоне с помощью примитивов (кнопки, поля, любой html) -- а потом этот код выполняется на сервере и обслуживает каждого пользователя в отдельном процессе.\n",
        "\n",
        "Давайте разберём [базовый пример с семинара](https://huggingface.co/spaces/justheuristic/TODO_YSDA_TEST/tree/main). Он состоит из 2 файлов: [app.py](https://huggingface.co/spaces/justheuristic/TODO_YSDA_TEST/blob/main/app.py) и [requirements.txt](https://huggingface.co/spaces/justheuristic/TODO_YSDA_TEST/blob/main/requirements.txt)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97036d5f",
      "metadata": {
        "id": "97036d5f"
      },
      "source": [
        "`app.py` это питонячий код, который будет выполняться на _каждое_ действие пользователя: когда новый пользователь открывает страницу, когда кто-то меняет текст или нажимает на кнопку, етц. В нашем случае:\n",
        "```python\n",
        "import streamlit as st\n",
        "\n",
        "st.markdown(\"### Hello, world!\")\n",
        "st.markdown(\"<img width=200px src='https://rozetked.me/images/uploads/dwoilp3BVjlE.jpg'>\", unsafe_allow_html=True)\n",
        "# ^-- можно показывать пользователю текст, картинки, ограниченное подмножество html - всё как в jupyter\n",
        "\n",
        "text = st.text_area(\"TEXT HERE\")\n",
        "# ^-- показать текстовое поле. В поле text лежит строка, которая находится там в данный момент\n",
        "\n",
        "from transformers import pipeline\n",
        "pipe = pipeline(\"ner\", \"Davlan/distilbert-base-multilingual-cased-ner-hrl\")\n",
        "raw_predictions = pipe(text)\n",
        "# тут уже знакомый вам код с huggingface.transformers -- его можно заменить на что угодно от fairseq до catboost\n",
        "\n",
        "st.markdown(f\"{raw_predictions}\")\n",
        "# выводим результаты модели в текстовое поле, на потеху пользователю\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a421786",
      "metadata": {
        "id": "0a421786"
      },
      "source": [
        "\n",
        "__Для отладки__ можно запустить приложение локально, открыв консоль рядом с app.py и...\n",
        "* `pip install streamlit`\n",
        "* `streamlit run app.py --server.port 8080`\n",
        "* открыть в браузере localhost:8080, если он не открылся автоматически\n",
        "\n",
        "Вы увидите такую картину (текст про кота дописан в браузере): ![img](https://i.imgur.com/DYa06BB.png)\n",
        "\n",
        "\n",
        "### Deployment time!\n",
        "\n",
        "В этот раз вам нужно не просто написать код, __но и поднять ваше приложение с доступом из интернета__. И да, вы угадали, это можно сделать несколькими способами: [HuggingFace spaces](https://huggingface.co/spaces), [Streamlit Cloud](https://streamlit.io/cloud), а ещё вы можете купить или арендовать свой собственный сервер и захоститься там.\n",
        "\n",
        "`[дальше будет текст про HF spaces, но если вам лень его смотреть, то вот `[`[видео]`](https://youtu.be/3bSVKNKb_PY)]`]`\n",
        "\n",
        "Проще всего захостить на HF spaces, для этого вам нужно [зарегистрироваться](https://huggingface.co/join) и найти [меню создания нового приложения](https://huggingface.co/new-space). Название и лицензию можно выбрать на своё усмотрение, главное чтобы Space SDK был Streamlit, а доступ - public.\n",
        "\n",
        "Как создали - можно редактировать ваше приложение прямо на сайте, для этого откройте приложение и перейдите в Files and versions, и там в правом углу добавьте нужные файлы:\n",
        "![img](https://i.imgur.com/6O8KV1Q.png)\n",
        "\n",
        "На минималках вам потребуется 2 файла:\n",
        "- `app.py`, о котором мы говорили выше\n",
        "- `requirements.txt`, где вы укажете нужные вам библиотеки ([пример](https://huggingface.co/spaces/justheuristic/TODO_YSDA_TEST/blob/main/requirements.txt))\n",
        "\n",
        "Вы можете разместить там же веса вашей обученной модели с помощью `torch.save(..., model.state_dict())` ([подробнее про torch.save](https://pytorch.org/docs/stable/notes/serialization.html)) и любые дополнительные данные.\n",
        "\n",
        "После каждого изменения файлов, ваше приложение соберётся (обычно 1-5 минут) и будет доступно уже во вкладке App. Ну или не соберётся и покажет вам, где оно сломалось. И вуаля, теперь у вас есть ссылка, которую можно показать ~друзьям~ ассистентам курса и кому угодно в интернете."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c847d2ff",
      "metadata": {
        "id": "c847d2ff"
      },
      "source": [
        "### И что теперь?\n",
        "\n",
        "Пример, который мы только что разобрали - он скорее простой, чем хороший. Есть три очевидных места, которые вы можете улучшить у себя в приложении:\n",
        "\n",
        "__1. Скорость работы.__ По умолчанию, streamlit будет выполняет весь ваш код на каждое действие пользователя. То есть всякий раз, когда пользователь меняет что-то в тексте, оно будет заново загружать модель. Чтобы исправить это безобразие, вы можете закэшировать подготовленную модель в `@st.cache`. Подробности [читайте тут](https://docs.streamlit.io/library/advanced-features/caching).\n",
        "\n",
        "__Как будет оцениваться:__ вы не обязаны пользоваться кэшированием, но ваше приложение не должно неоправдано тормозить дольше, чем на 3 секунды. \"Оправданые\" тормоза это те, которые вы явно оправдали текстом в ЛМС :)\n",
        "\n",
        "__2. Понятный фронтенд.__ графический интерфейс примера - пример того, как не надо делать интерфейс приложения. Как надо - сложный вопрос, причём настолько сложный, что есть даже [Школа Разработки Интерфейсов](https://academy.yandex.ru/schools/frontend). Но для начала:\n",
        "\n",
        "- Выводить нужно человекочитаемый текст, а не просто JSON с индексами и метаданными.\n",
        "- Пользователю должно быть понятно, куда и какие данные вводить. Пустые текстовые поля в вакууме - плохой тон.\n",
        "- Сервис не должен падать с не_отловленными ошибками. Даже если пользователь введёт неправильные/пустые данные, нужно это обработать и написать, где произошла ошибка.\n",
        "\n",
        "__Как будет оцениваться:__ для полного балла достаточно соблюсти эти три правила и специально не стрелять себе в ногу. Картинку с котиком тоже лучше заменить на что-то более уместное.\n",
        "\n",
        "__3. Удобная работа с кодом.__ Пока у вас 2 файла, их легко редактивровать прямо в интерфейсе HF spaces. Если же у вас дюжина файлов, вам может быть удобнее редактировать их в любимом vscode/pycharm/.../emacs. Чтобы это не вызывало мучений, можно пользоваться HF spaces как git репозиторием ([подробности тут](https://huggingface.co/docs/hub/spaces#manage-app-with-github-actions)).\n",
        "\n",
        "__Как будет оцениваться:__ а никак, пишите код как хотите.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Kt94ld4xQRgN"
      },
      "id": "Kt94ld4xQRgN",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "hw_transformers_devops_Erin_upd.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b3efc33e5247459bb8da664ecd8d2fd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e7cf0e45cca74f86b1dfca1948ad1abc",
              "IPY_MODEL_1cdef38128d245d5a56c6cd2e05d7b45",
              "IPY_MODEL_da2e20ec8d444d03b5195a70e2ba37b5"
            ],
            "layout": "IPY_MODEL_a43c997f6ef84a5eb70c0643f0857edf"
          }
        },
        "e7cf0e45cca74f86b1dfca1948ad1abc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac44ad5c0c6c413f9e156ba64965ae3b",
            "placeholder": "​",
            "style": "IPY_MODEL_7b352c00b1c34a0888fbb08026aab67c",
            "value": "100%"
          }
        },
        "1cdef38128d245d5a56c6cd2e05d7b45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abb931957b0d474eb4da33dda4a4dc40",
            "max": 9,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_998de46b427546689338992f20047277",
            "value": 9
          }
        },
        "da2e20ec8d444d03b5195a70e2ba37b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97bbc008970a4639bc5cdcd32c90344e",
            "placeholder": "​",
            "style": "IPY_MODEL_2fc8ee670168419594b5a5c100ce7a3b",
            "value": " 9/9 [00:05&lt;00:00,  1.85ba/s]"
          }
        },
        "a43c997f6ef84a5eb70c0643f0857edf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac44ad5c0c6c413f9e156ba64965ae3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b352c00b1c34a0888fbb08026aab67c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abb931957b0d474eb4da33dda4a4dc40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "998de46b427546689338992f20047277": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "97bbc008970a4639bc5cdcd32c90344e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fc8ee670168419594b5a5c100ce7a3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0824eb29213403e9a00f473f02846df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c77be41a1d114e02a41c6cf6de759a85",
              "IPY_MODEL_588e0f2b7f834baeb89f4f4f60c8b9c1",
              "IPY_MODEL_48c476d27c9b47e3b84609c7e6af326f"
            ],
            "layout": "IPY_MODEL_ec136ccd3eb44544b031a0d9e5e1d204"
          }
        },
        "c77be41a1d114e02a41c6cf6de759a85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f750c1adb4c1446b96948acb9d013875",
            "placeholder": "​",
            "style": "IPY_MODEL_ed44db8295f14d51806993c7af894545",
            "value": "100%"
          }
        },
        "588e0f2b7f834baeb89f4f4f60c8b9c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e547fddd3a84595b4dd58db026fc663",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_555420acf21e452d8f9ba9a82e3f828f",
            "value": 3
          }
        },
        "48c476d27c9b47e3b84609c7e6af326f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bf76ee485124fb989d121412986b5e5",
            "placeholder": "​",
            "style": "IPY_MODEL_4bbf8a66589b4bcf94466848a244cb9c",
            "value": " 3/3 [00:01&lt;00:00,  2.66ba/s]"
          }
        },
        "ec136ccd3eb44544b031a0d9e5e1d204": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f750c1adb4c1446b96948acb9d013875": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed44db8295f14d51806993c7af894545": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e547fddd3a84595b4dd58db026fc663": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "555420acf21e452d8f9ba9a82e3f828f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1bf76ee485124fb989d121412986b5e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bbf8a66589b4bcf94466848a244cb9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}