{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zriTdjauH8iQ"},"outputs":[],"source":["!pip install transformers\n","import transformers"]},{"cell_type":"markdown","metadata":{"id":"xQiRPWWHlSgv"},"source":["### Using pre-trained transformers\n","_for fun and profit_\n","\n","There are many toolkits that let you access pre-trained transformer models, but the most powerful and convenient by far is [`huggingface/transformers`](https://github.com/huggingface/transformers). In this week's practice, you'll learn how to download, apply and modify pre-trained transformers for a range of tasks. Buckle up, we're going in!\n","\n","\n","__Pipelines:__ if all you want is to apply a pre-trained model, you can do that in one line of code using pipeline. Huggingface/transformers has a selection of pre-configured pipelines for masked language modelling, sentiment classification, question aswering, etc. ([see full list here](https://huggingface.co/transformers/main_classes/pipelines.html))\n","\n","A typical pipeline includes:\n","* pre-processing, e.g. tokenization, subword segmentation\n","* a backbone model, e.g. bert finetuned for classification\n","* output post-processing\n","\n","Let's see it in action:"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"rP1KFtvLlJHR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648409760704,"user_tz":-180,"elapsed":8955,"user":{"displayName":"Fedor Erin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3PQgFbGKgh9V7jO3Oyk7BfT0EX5ddd9R-nNJBrQ=s64","userId":"14230325476000231713"}},"outputId":"c0ac1e5e-dd9e-44c2-93fd-3303a77b3b8b"},"outputs":[{"output_type":"stream","name":"stdout","text":["[{'label': 'POSITIVE', 'score': 0.9998860359191895}]\n"]}],"source":["from transformers import pipeline\n","classifier = pipeline('sentiment-analysis', model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n","\n","print(classifier(\"BERT is amazing!\"))"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"nYUNuyXMn5l9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648409794846,"user_tz":-180,"elapsed":316,"user":{"displayName":"Fedor Erin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3PQgFbGKgh9V7jO3Oyk7BfT0EX5ddd9R-nNJBrQ=s64","userId":"14230325476000231713"}},"outputId":"a7d3382f-754e-47b6-815d-e7034d9b5ae6"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'arryn': True, 'baratheon': False, 'stark': True, 'tyrell': True}\n","Well done!\n"]}],"source":["import base64\n","data = {\n","    'arryn': 'As High as Honor.',\n","    'baratheon': 'Ours is the fury.',\n","    'stark': 'Winter is coming.',\n","    'tyrell': 'Growing strong.'\n","}\n","\n","# YOUR CODE: predict sentiment for each noble house and create outputs dict\n","# outputs = <YOUR CODE: dict (house name) : True if positive, False if negative>\n","\n","outputs = {}\n","for k, v in data.items():\n","    label = classifier(v)[0]['label']\n","    outputs[k] = True if label == 'POSITIVE' else False\n","print(outputs)\n","\n","assert sum(outputs.values()) == 3 and outputs[base64.decodebytes(b'YmFyYXRoZW9u\\n').decode()] == False\n","print(\"Well done!\")"]},{"cell_type":"markdown","metadata":{"id":"BRDhIH-XpSNo"},"source":["You can also access vanilla Masked Language Model that was trained to predict masked words. Here's how:"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"pa-8noIllRbZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648409802950,"user_tz":-180,"elapsed":4334,"user":{"displayName":"Fedor Erin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3PQgFbGKgh9V7jO3Oyk7BfT0EX5ddd9R-nNJBrQ=s64","userId":"14230325476000231713"}},"outputId":"bfdf617f-a1a7-4f29-c2b0-dfdb593afaa5"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["P=0.99719 donald trump is the president of the united states.\n","P=0.00024 donald duck is the president of the united states.\n","P=0.00022 donald ross is the president of the united states.\n","P=0.00020 donald johnson is the president of the united states.\n","P=0.00018 donald wilson is the president of the united states.\n"]}],"source":["mlm_model = pipeline('fill-mask', model=\"bert-base-uncased\")\n","MASK = mlm_model.tokenizer.mask_token\n","\n","for hypo in mlm_model(f\"Donald {MASK} is the president of the united states.\"):\n","  print(f\"P={hypo['score']:.5f}\", hypo['sequence'])"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"9NxeG1Y5pwX1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648409826682,"user_tz":-180,"elapsed":227,"user":{"displayName":"Fedor Erin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3PQgFbGKgh9V7jO3Oyk7BfT0EX5ddd9R-nNJBrQ=s64","userId":"14230325476000231713"}},"outputId":"2ef0dea4-8a77-4c5a-cb55-5ce6e483d06a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'score': 0.05159085988998413,\n","  'sequence': 'what year was the soviet union founded in? it was founded in october.',\n","  'token': 2255,\n","  'token_str': 'october'},\n"," {'score': 0.04497303441166878,\n","  'sequence': 'what year was the soviet union founded in? it was founded in russia.',\n","  'token': 3607,\n","  'token_str': 'russia'},\n"," {'score': 0.02986496314406395,\n","  'sequence': 'what year was the soviet union founded in? it was founded in moscow.',\n","  'token': 4924,\n","  'token_str': 'moscow'},\n"," {'score': 0.028700895607471466,\n","  'sequence': 'what year was the soviet union founded in? it was founded in february.',\n","  'token': 2337,\n","  'token_str': 'february'},\n"," {'score': 0.0274384506046772,\n","  'sequence': 'what year was the soviet union founded in? it was founded in november.',\n","  'token': 2281,\n","  'token_str': 'november'}]"]},"metadata":{},"execution_count":7}],"source":["# Your turn: use bert to recall what year was the Soviet Union founded in\n","# mlm_model(<YOUR PROMPT>)\n","mlm_model(f\"What year was the Soviet Union founded in? It was founded in {MASK}.\")"]},{"cell_type":"markdown","metadata":{"id":"YJxRFzCSq903"},"source":["```\n","\n","```\n","\n","```\n","\n","```\n","\n","\n","Huggingface offers hundreds of pre-trained models that specialize on different tasks. You can quickly find the model you need using [this list](https://huggingface.co/models).\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"HRux8Qp2hkXr","executionInfo":{"status":"ok","timestamp":1648409863363,"user_tz":-180,"elapsed":4002,"user":{"displayName":"Fedor Erin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3PQgFbGKgh9V7jO3Oyk7BfT0EX5ddd9R-nNJBrQ=s64","userId":"14230325476000231713"}}},"outputs":[],"source":["text = \"\"\"Almost two-thirds of the 1.5 million people who viewed this liveblog had Googled to discover\n"," the latest on the Rosetta mission. They were treated to this detailed account by the Guardian’s science editor,\n"," Ian Sample, and astronomy writer Stuart Clark of the moment scientists landed a robotic spacecraft on a comet \n"," for the first time in history, and the delirious reaction it provoked at their headquarters in Germany.\n","  “We are there. We are sitting on the surface. Philae is talking to us,” said one scientist.\n","\"\"\"\n","\n","# Task: create a pipeline for named entity recognition, use task name 'ner' and search for the right model in the list\n","# ner_model = <YOUR CODE>\n","ner_model = pipeline('ner', model=\"dslim/bert-base-NER\")\n","\n","named_entities = ner_model(text)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"hf57MRzSiSON","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648409871641,"user_tz":-180,"elapsed":254,"user":{"displayName":"Fedor Erin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3PQgFbGKgh9V7jO3Oyk7BfT0EX5ddd9R-nNJBrQ=s64","userId":"14230325476000231713"}},"outputId":"d8db40ac-7df0-48d0-a060-a4c1ec22bb0e"},"outputs":[{"output_type":"stream","name":"stdout","text":["OUTPUT: [{'entity': 'B-LOC', 'score': 0.799105, 'index': 27, 'word': 'Rose', 'start': 112, 'end': 116}, {'entity': 'I-LOC', 'score': 0.95119274, 'index': 28, 'word': '##tta', 'start': 116, 'end': 119}, {'entity': 'B-ORG', 'score': 0.998223, 'index': 40, 'word': 'Guardian', 'start': 179, 'end': 187}, {'entity': 'B-PER', 'score': 0.9997613, 'index': 46, 'word': 'Ian', 'start': 207, 'end': 210}, {'entity': 'I-PER', 'score': 0.99978715, 'index': 47, 'word': 'Sam', 'start': 211, 'end': 214}, {'entity': 'I-PER', 'score': 0.99964595, 'index': 48, 'word': '##ple', 'start': 214, 'end': 217}, {'entity': 'B-PER', 'score': 0.9997831, 'index': 53, 'word': 'Stuart', 'start': 240, 'end': 246}, {'entity': 'I-PER', 'score': 0.9997482, 'index': 54, 'word': 'Clark', 'start': 247, 'end': 252}, {'entity': 'B-LOC', 'score': 0.9997228, 'index': 85, 'word': 'Germany', 'start': 414, 'end': 421}, {'entity': 'B-PER', 'score': 0.9963127, 'index': 99, 'word': 'Phil', 'start': 471, 'end': 475}, {'entity': 'I-PER', 'score': 0.98892516, 'index': 100, 'word': '##ae', 'start': 475, 'end': 477}]\n","All tests passed\n"]}],"source":["print('OUTPUT:', named_entities)\n","word_to_entity = {item['word']: item['entity'] for item in named_entities}\n","assert 'org' in word_to_entity.get('Guardian').lower() and 'per' in word_to_entity.get('Stuart').lower()\n","print(\"All tests passed\")"]},{"cell_type":"markdown","metadata":{"id":"ULMownz6sP9n"},"source":["### The building blocks of a pipeline\n","\n","Huggingface also allows you to access its pipelines on a lower level. There are two main abstractions for you:\n","* `Tokenizer` - converts from strings to token ids and back\n","* `Model` - a pytorch `nn.Module` with pre-trained weights\n","\n","You can use such models as part of your regular pytorch code: insert is as a layer in your model, apply it to a batch of data, backpropagate, optimize, etc."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"KMJbV0QVsO0Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648409940471,"user_tz":-180,"elapsed":3667,"user":{"displayName":"Fedor Erin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3PQgFbGKgh9V7jO3Oyk7BfT0EX5ddd9R-nNJBrQ=s64","userId":"14230325476000231713"}},"outputId":"c87088ea-aa55-4efd-a447-91249c050f0e"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModel, pipeline\n","\n","model_name = 'bert-base-uncased'\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModel.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"ZgSPHKPRxG6U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648409965813,"user_tz":-180,"elapsed":6,"user":{"displayName":"Fedor Erin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3PQgFbGKgh9V7jO3Oyk7BfT0EX5ddd9R-nNJBrQ=s64","userId":"14230325476000231713"}},"outputId":"dd721a59-eeb0-4fc0-8c37-3352e6a4e1da"},"outputs":[{"output_type":"stream","name":"stdout","text":["input_ids tensor([[ 101, 5355, 1010, 1045, 2572, 2115, 2269, 1012,  102,    0,    0,    0,\n","            0,    0,    0],\n","        [ 101, 2166, 2003, 2054, 6433, 2043, 2017, 1005, 2128, 5697, 2437, 2060,\n","         3488, 1012,  102]])\n","token_type_ids tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n","attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Detokenized:\n","[CLS] luke, i am your father. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","[CLS] life is what happens when you're busy making other plans. [SEP]\n"]}],"source":["lines = [\n","    \"Luke, I am your father.\",\n","    \"Life is what happens when you're busy making other plans.\",\n","    ]\n","\n","# tokenize a batch of inputs. \"pt\" means [p]y[t]orch tensors\n","tokens_info = tokenizer(lines, padding=True, truncation=True, return_tensors=\"pt\")\n","\n","for key in tokens_info:\n","    print(key, tokens_info[key])\n","\n","print(\"Detokenized:\")\n","for i in range(2):\n","    print(tokenizer.decode(tokens_info['input_ids'][i]))"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"MJkbHxERyfL4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648409969843,"user_tz":-180,"elapsed":313,"user":{"displayName":"Fedor Erin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3PQgFbGKgh9V7jO3Oyk7BfT0EX5ddd9R-nNJBrQ=s64","userId":"14230325476000231713"}},"outputId":"468316a9-254f-4b6d-e463-90090ea09175"},"outputs":[{"output_type":"stream","name":"stdout","text":["BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.3502,  0.2246, -0.2345,  ..., -0.2232,  0.1730,  0.6747],\n","         [-0.6097,  0.6892, -0.5512,  ..., -0.4814,  0.5322,  1.3833],\n","         [ 0.1842,  0.4881,  0.2193,  ..., -0.2699,  0.2246,  0.7985],\n","         ...,\n","         [-0.4413,  0.2748, -0.0391,  ..., -0.0604, -0.4358,  0.1384],\n","         [-0.5414,  0.4633,  0.0678,  ..., -0.1871, -0.5046,  0.2752],\n","         [-0.3940,  0.6180,  0.2092,  ..., -0.2345, -0.4177,  0.3341]],\n","\n","        [[ 0.1622, -0.1154, -0.3894,  ..., -0.4180,  0.0138,  0.7644],\n","         [ 0.6471,  0.3774, -0.4082,  ...,  0.0050,  0.5559,  0.4385],\n","         [ 0.3351, -0.3158, -0.1178,  ...,  0.1348, -0.3143,  1.4409],\n","         ...,\n","         [ 1.2932, -0.1743, -0.5613,  ..., -0.2718, -0.1367,  0.4217],\n","         [ 1.0304,  0.1708, -0.2985,  ...,  0.2097, -0.4627, -0.4277],\n","         [ 1.0854,  0.1760, -0.0377,  ...,  0.3152, -0.5979, -0.3465]]]), pooler_output=tensor([[-0.8854, -0.4722, -0.9392,  ..., -0.8081, -0.6955,  0.8748],\n","        [-0.9297, -0.5161, -0.9334,  ..., -0.9017, -0.7492,  0.9201]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"]}],"source":["# You can now apply the model to get embeddings\n","with torch.no_grad():\n","    out = model(**tokens_info)\n","\n","print(out)"]},{"cell_type":"code","source":["isinstance(model, torch.nn.Module)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rv15M_UarTOl","executionInfo":{"status":"ok","timestamp":1648409986750,"user_tz":-180,"elapsed":230,"user":{"displayName":"Fedor Erin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3PQgFbGKgh9V7jO3Oyk7BfT0EX5ddd9R-nNJBrQ=s64","userId":"14230325476000231713"}},"outputId":"aa3d2824-cb99-48df-a9a8-d50a90d5cacc"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"r5Yzlij6IgnS"},"source":["### Training\n","\n","As you have just seen, HF models can be used like any other pytorch module. Hence, you can train them normally by writing a custom loop. However, HF also has a (mostly) convenient Trainer interface that can do that for you:"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"32nlflQiIgnU","outputId":"a976830e-35bc-44c6-e3d2-c00a25bd63f4","executionInfo":{"status":"ok","timestamp":1648409993106,"user_tz":-180,"elapsed":3805,"user":{"displayName":"Fedor Erin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3PQgFbGKgh9V7jO3Oyk7BfT0EX5ddd9R-nNJBrQ=s64","userId":"14230325476000231713"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["staff behind the deli counter were super nice and efficient !\n","love this place !\n","the staff are always very nice and helpful .\n","the new yorker was amazing .\n","very ny style italian deli .\n","\n","ok never going back to this place again .\n","easter day nothing open , heard about this place figured it would ok .\n","the host that walked us to the table and left without a word .\n","it just gets worse .\n","the food tasted awful .\n"]}],"source":["!wget -q https://github.com/shentianxiao/language-style-transfer/raw/master/data/yelp/sentiment.train.0 -O train_negative\n","!wget -q https://github.com/shentianxiao/language-style-transfer/raw/master/data/yelp/sentiment.train.1 -O train_positive\n","!wget -q https://github.com/shentianxiao/language-style-transfer/raw/master/data/yelp/sentiment.dev.0 -O dev_negative\n","!wget -q https://github.com/shentianxiao/language-style-transfer/raw/master/data/yelp/sentiment.dev.1 -O dev_positive\n","\n","!head -n 5 ./dev_positive\n","!echo\n","!head -n 5 ./dev_negative"]},{"cell_type":"code","source":["from transformers import AutoModelForSequenceClassification\n","\n","model = AutoModelForSequenceClassification.from_pretrained(model_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dTFq9Byivy6n","executionInfo":{"status":"ok","timestamp":1648410008615,"user_tz":-180,"elapsed":2340,"user":{"displayName":"Fedor Erin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3PQgFbGKgh9V7jO3Oyk7BfT0EX5ddd9R-nNJBrQ=s64","userId":"14230325476000231713"}},"outputId":"6cdef028-280c-4da9-e1c6-7206ded57725"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","execution_count":16,"metadata":{"id":"DkBWiEiyIgnV","colab":{"base_uri":"https://localhost:8080/","height":664},"executionInfo":{"status":"error","timestamp":1648410048723,"user_tz":-180,"elapsed":23704,"user":{"displayName":"Fedor Erin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3PQgFbGKgh9V7jO3Oyk7BfT0EX5ddd9R-nNJBrQ=s64","userId":"14230325476000231713"}},"outputId":"28e416eb-9462-45e0-a37f-fd30139f9780"},"outputs":[{"output_type":"stream","name":"stdout","text":["Preparing the training data...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:125: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Dataset ready!\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 38205\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 1\n","  Total train batch size (w. parallel, distributed & accumulation) = 1\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 38205\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='142' max='38205' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [  142/38205 00:17 < 1:19:47, 7.95 it/s, Epoch 0.00/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-14d4eeef1532>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1443\u001b[0m                             nn.utils.clip_grad_norm_(\n\u001b[1;32m   1444\u001b[0m                                 \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_apex\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1445\u001b[0;31m                                 \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1446\u001b[0m                             )\n\u001b[1;32m   1447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mclip_coef_clamped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_coef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_coef_clamped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from transformers import Trainer, TrainingArguments, LineByLineTextDataset\n","\n","print(\"Preparing the training data...\")\n","# dataset = LineByLineTextDataset(\n","#     file_path=<MAKE_YOUR_DATA_HERE>, tokenizer=tokenizer, block_size=128)\n","data_pos = LineByLineTextDataset(file_path='./dev_positive', tokenizer=tokenizer, block_size=128)\n","\n","print(\"Dataset ready!\")\n","# dataset = dataset.map(lambda xs: tokenizer(xs[\"text\"], truncation=True))\n","data_pos = data_pos.map(lambda xs: dict(input_ids=xs[\"input_ids\"], labels=1))\n","\n","trainer = Trainer(\n","    model=model, train_dataset=data_pos,\n","    args=TrainingArguments(\n","        output_dir=\"./my_saved_model\", overwrite_output_dir=True,\n","        num_train_epochs=1, per_device_train_batch_size=1,\n","        save_steps=10_000, save_total_limit=2),\n",")\n","\n","trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"OHEC6o7uAfgQ"},"source":["```\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","```\n","\n","__Bonus demo:__ transformer language models. \n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"vWCajBGcAern","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648410089217,"user_tz":-180,"elapsed":3558,"user":{"displayName":"Fedor Erin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3PQgFbGKgh9V7jO3Oyk7BfT0EX5ddd9R-nNJBrQ=s64","userId":"14230325476000231713"}},"outputId":"44df7f6b-ebe4-4306-833f-5f3fc5599924"},"outputs":[{"output_type":"stream","name":"stderr","text":["loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n","loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n","loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n","Model config GPT2Config {\n","  \"_name_or_path\": \"gpt2\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 50256,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.17.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257\n","}\n","\n","loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n","Model config GPT2Config {\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 50256,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.17.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257\n","}\n","\n","loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"]}],"source":["import torch\n","import numpy as np\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2', add_prefix_space=True)\n","model = GPT2LMHeadModel.from_pretrained('gpt2').train(False).to(device)\n","\n","def run_gpt2_for_text(text):\n","    tokens = tokenizer.encode(text)\n","    num_steps = 1024\n","    line_length, max_length = 0, 70\n","\n","    print(end=tokenizer.decode(tokens))\n","\n","    for i in range(num_steps):\n","        with torch.no_grad():\n","            logits = model(torch.as_tensor([tokens], device=device))[0]\n","        # p_next = torch.softmax(logits[0, -1, :], dim=-1).data.cpu().numpy()\n","        p_next = torch.softmax(logits[0, -1, :] / 0.8, dim=-1).data.cpu().numpy()\n","\n","        # next_token_index = p_next.argmax() #<YOUR CODE: REPLACE THIS LINE>\n","        # YOUR TASK: change the code so that it performs nucleus sampling\n","        next_token_index = np.random.choice(len(p_next), p=p_next)\n","\n","        tokens.append(int(next_token_index))\n","        print(end=tokenizer.decode(tokens[-1]))\n","        line_length += len(tokenizer.decode(tokens[-1]))\n","        if line_length >= max_length:\n","            line_length = 0\n","            print()"]},{"cell_type":"code","source":["text = \"The Fermi paradox \"\n","run_gpt2_for_text(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":722},"id":"l1ANU6XqzL28","executionInfo":{"status":"error","timestamp":1648410108579,"user_tz":-180,"elapsed":13260,"user":{"displayName":"Fedor Erin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3PQgFbGKgh9V7jO3Oyk7BfT0EX5ddd9R-nNJBrQ=s64","userId":"14230325476000231713"}},"outputId":"1531e1dc-2ebd-4f7a-d1ed-a6ff8f77601e"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":[" The Fermi paradox  draws us to a more troubling conclusion:\n","Something else is at work in\n"," the political world. The fact that in many cases the economic and political\n"," system of governments fails to account for the economic and political\n"," system of the working class is not a coincidence. Politicians have funded\n"," a lot of our political activities by means of money, since the bourgeoisie\n"," has been a Christian religion for centuries, and the bourgeoisie has been\n"," a Christian religion for centuries and has been the foundation of political\n"," life for almost a hundred years. The traditional conception of the patron\n","cias of the bourgeoisie was that they would have nothing to do with the\n"," state, that they would be able to make the state operate for them as they\n"," wished without their authority, and that they would be able to make the\n"," state administer to them their own interests. But this first-century thought\n"," of the patroncias as the ultimate social agents and governors of the state\n"," comes from a period when the ruling class in Britain could have no power\n"," over its own interests. The main reflection of the political state, to\n"," which it belongs, was the idea that it should not be a social body, such\n"," as an institution which could both receive and distribute the same resources\n",", so long as it \"employed\" without interfering with the political authority\n"," of the state and could only be effective insofar as it employed its own\n"," interests. The patroncias began to form their own popular movements after\n"," the English Civil War, and they were soon the first groups to call for\n"," an"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-cae52786a4e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The Fermi paradox \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun_gpt2_for_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-17-c2c9d01bf8ea>\u001b[0m in \u001b[0;36mrun_gpt2_for_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# p_next = torch.softmax(logits[0, -1, :], dim=-1).data.cpu().numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mp_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# next_token_index = p_next.argmax() #<YOUR CODE: REPLACE THIS LINE>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"_Vij7Gc1wOaq"},"source":["Transformers knowledge hub: https://huggingface.co/transformers/"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"ysda_2022_03_07_transformers.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}